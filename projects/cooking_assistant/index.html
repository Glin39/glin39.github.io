<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Context Aware Cooking Assistant | Blue (Georgianna) Lin</title> <meta name="author" content="Blue (Georgianna) Lin"> <meta name="description" content="Designing a smart assistant to support continuous context switches in cooking by understanding user progress and environment through multimodal sensing"> <meta name="keywords" content="health tracking, women's health, health sensing, ubiquitous devices, user-centered design, social good"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%B1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://glin39.github.io/projects/cooking_assistant/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Blue¬†</span>(Georgianna)¬†Lin</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Context Aware Cooking Assistant</h1> <p class="post-description">Designing a smart assistant to support continuous context switches in cooking by understanding user progress and environment through multimodal sensing</p> </header> <article> <div class="portfolio-case-study"> <section class="tldr-hero"> <div class="tldr-badge">TL;DR</div> <h1 class="hero-title">Deriving Design Needs for AI Cooking Assistants</h1> <div class="hero-subtitle"> <p>Cooking involves constant context switching between understanding instructions and performing physical tasks in real-time. This research explored how to design multimodal cooking assistants that understand user progress, environmental context, and support natural interaction flows.</p> </div> <div class="demo-section"> <h3>Research Demo Video</h3> <div class="video-container"> <iframe width="100%" height="315" src="https://www.youtube.com/embed/H57S8TY0hTA" title="Context Aware Cooking Assistant Research Demo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""> </iframe> </div> </div> </section> <section class="problem-section"> <div class="section-header"> <h2>The Problem</h2> </div> <div class="problem-content"> <div class="problem-statement"> <h3>Cooking assistance requires understanding context, not just controlling media</h3> <p>Following cooking videos requires users to constantly alternate attention between understanding video instructions and performing those steps in their physical environment. Current video navigation tools treat this as a simple media control problem, ignoring the rich contextual information available in the cooking environment.</p> </div> <div class="problem-details"> <div class="problem-point"> <h4>Context switching creates cognitive overhead</h4> <p>Users must mentally track their progress, map video instructions to their specific setup, and troubleshoot differences between what they see on screen vs. their kitchen reality.</p> </div> <div class="problem-point"> <h4>How to design effective systems</h4> <p>Many existing cooking assistants and research efforts focus on agents that only process video. However, with recent advances in intelligent multimodal agent systems, we can now envision agents that offer contextually relevant assistance. The challenge lies in how to design such systems effectively.</p> </div> </div> </div> </section> <section class="role-section"> <div class="section-header"> <h2>What I Did</h2> </div> <div class="role-grid"> <div class="role-card"> <div class="role-icon">üî¨</div> <h4>Wizard-of-Oz Study Design</h4> <p>Designed and conducted a controlled study simulating AI-powered contextual assistance through human operators</p> </div> <div class="role-card"> <div class="role-icon">üìä</div> <h4>Interaction Analysis</h4> <p>Analyzed 30+ hours of cooking sessions to understand query patterns, workflow alignment, and assistant needs</p> </div> <div class="role-card"> <div class="role-icon">üé®</div> <h4>Design Framework</h4> <p>Derived design principles for context-aware assistants that extend beyond the cooking domain</p> </div> </div> </section> <section class="approach-section"> <div class="section-header"> <h2>User Study</h2> </div> <div class="media-placeholder media-placeholder-large"> <img src="/assets/img/cooking.png" alt="Wizard-of-oz study setup showing researcher and participant perspectives"> <p>Wizard-of-oz study setup: researcher could see participant's cooking environment while controlling shared video interface</p> </div> <div class="approach-content"> <div class="research-goal"> <h3>Research Goal</h3> <p class="goal-text">How can we design context-aware assistants that support users as they switch focus between following procedural video instructions and performing cooking tasks in real-time?</p> <ul class="goal-questions"> <li>What types of contextual information enable more effective cooking assistance?</li> <li>How do users naturally interact with environment-aware assistants during hands-on tasks?</li> <li>What design patterns from cooking assistance can generalize to other procedural domains?</li> </ul> </div> <div class="two-column"> <div class="method-card"> <div class="method-title">Study Protocol</div> <p><strong>Video conference setup:</strong> Participants joined from home kitchens, researchers controlled shared video screen</p> <p><strong>Environmental awareness simulation:</strong> Researchers could see participant's cooking space and context in real-time</p> <p><strong>Natural interaction:</strong> Participants encouraged to ask any questions they would want from an ideal cooking assistant</p> </div> <div class="method-card"> <div class="method-title">Data Capture</div> <p><strong>Multi-stream recording:</strong> User video, audio, all verbal interactions, and environmental sounds</p> <p><strong>Recipe selection:</strong> Participants chose unfamiliar but interesting dishes from 5 options, selected own YouTube videos</p> <p><strong>Session structure:</strong> 1-hour cooking sessions with pre/post surveys and demographic data</p> </div> </div> <div class="stats-grid"> <div class="stat-card"> <span class="stat-number">30</span> <div class="stat-label">Participants (average-intermediate cooks)</div> </div> <div class="stat-card"> <span class="stat-number">5</span> <div class="stat-label">Standardized recipe categories</div> </div> <div class="stat-card"> <span class="stat-number">100%</span> <div class="stat-label">Home kitchen environments</div> </div> </div> <div class="design-decision"> <div class="design-decision-label"> <strong>‚ö†Ô∏è Technical Decision</strong> </div> <p>Used wizard-of-oz methodology rather than full AI implementation to focus on interaction patterns and user needs without being constrained by current AI limitations. This allowed us to simulate ideal contextual awareness and study user expectations.</p> </div> </div> </section> <section class="analysis-section"> <div class="section-header"> <h2>UX Analysis Framework</h2> </div> <div class="analysis-content"> <div class="analysis-process"> <div class="process-step"> <div class="step-number">01</div> <div class="step-content"> <h4>Open Coding &amp; Thematic Analysis</h4> <p>Two researchers independently performed line-by-line open coding on session transcripts and survey responses using established thematic analysis methods. Initial themes included "asking to replay video near end of cooking step" and "interactions requiring multimodal sensing."</p> </div> </div> <div class="process-step"> <div class="step-number">02</div> <div class="step-content"> <h4>Interaction Categorization Framework</h4> <p>Systematically grouped all user interactions into distinct categories based on intent, context, and required assistant capabilities. Categories emerged from data rather than predetermined frameworks.</p> <div class="media-placeholder media-placeholder-small"> <img src="/assets/img/ordered_manipulation.png" alt="Framework showing categorization of user interactions"> <p>Interaction categorization framework derived from thematic analysis</p> </div> </div> </div> <div class="process-step"> <div class="step-number">03</div> <div class="step-content"> <h4>User Persona Development</h4> <p>Used human-centered design methods to collaboratively build user personas based on cooking expertise, interaction patterns, and assistant usage preferences. Personas reached through researcher consensus and validation against participant data.</p> <div class="media-placeholder media-placeholder-small"> <img src="/assets/img/personas.png" alt="User personas derived from cooking interaction patterns"> <p>Data-driven user personas showing distinct interaction and assistance needs</p> </div> </div> </div> <div class="process-step"> <div class="step-number">04</div> <div class="step-content"> <h4>Task Flow &amp; Interaction Mapping</h4> <p>Derived typical cooking task flows and mapped where context switches and assistant interactions naturally occurred. Identified predictable patterns across different cooking styles and expertise levels.</p> <div class="media-placeholder media-placeholder-large"> <img src="/assets/img/big_flow.png" alt="Example cooking task flow showing context switch points"> <p>Example cooking task flow highlighting natural context switch points and assistant intervention opportunities</p> </div> </div> </div> <div class="process-step"> <div class="step-number">05</div> <div class="step-content"> <h4>Persona-Based Interaction Analysis</h4> <p>Created comprehensive interaction profiles by mapping each participant's query patterns to derived personas, revealing distinct usage patterns and assistant needs across user types.</p> <div class="media-placeholder media-placeholder-large"> <img src="/assets/img/interaction_persona.png" alt="Bar chart showing interaction patterns grouped by persona"> <p>Bar chart analysis: participant interactions categorized and grouped by derived personas</p> </div> </div> </div> </div> </div> </section> <section class="findings-section"> <div class="section-header"> <h2>Key Findings</h2> </div> <div class="insights-grid"> <div class="insight-card"> <div class="insight-number">1</div> <div class="insight-title">Users form distinct query patterns based on environmental context</div> <div class="insight-content"> <p>Analysis revealed three primary query categories that emerged naturally during context-aware cooking assistance:</p> <ul class="insight-list"> <li> <strong>Progress queries:</strong> "Did I add enough salt?" "Is this the right consistency?"</li> <li> <strong>Adaptation queries:</strong> "I don't have a stand mixer, what should I do?" "My pan is smaller than in the video"</li> <li> <strong>Timing queries:</strong> "How much longer should this simmer?" "When should I start the next step?"</li> </ul> <p>These query types rarely occur with traditional video controls, suggesting environmental awareness unlocks new interaction possibilities.</p> </div> </div> <div class="insight-card"> <div class="insight-number">2</div> <div class="insight-title">Workflow alignment varies dramatically between users despite similar goals</div> <div class="insight-content"> <p>While users had different interaction preferences and query patterns, their overall workflow structure showed surprising consistency:</p> <ul class="insight-list"> <li>All users followed similar preparation ‚Üí execution ‚Üí validation cycles</li> <li>Context switches occurred at predictable points (ingredient prep, technique changes, timing decisions)</li> <li>Environmental queries clustered around decision points and skill-adaptation moments</li> </ul> <p>This suggests context-aware systems can predict when assistance will be needed, even across diverse user preferences.</p> </div> </div> <div class="insight-card"> <div class="insight-number">3</div> <div class="insight-title">Multimodal sensing enables proactive rather than reactive assistance</div> <div class="insight-content"> <p>Environmental awareness allowed the system to anticipate user needs before explicit requests:</p> <ul class="insight-list"> <li>Computer vision detected when ingredients were missing or substituted</li> <li>Audio cues indicated technique struggles (e.g., inadequate whisking sounds)</li> <li>Sensor data revealed timing issues (oven preheating, pan temperature)</li> <li>Combined signals enabled contextually relevant suggestions at optimal moments</li> </ul> <p>Users reported feeling "understood" by the system rather than simply "controlled," leading to higher task confidence and learning outcomes.</p> </div> </div> </div> </section> <section class="next-steps-section"> <div class="section-header"> <h2>Design Implications &amp; Future Work</h2> </div> <div class="implications-table-container"> <table class="implications-table"> <thead> <tr> <th>Design Goal</th> <th>Technical Implementation</th> <th>Broader Applications</th> </tr> </thead> <tbody> <tr> <td class="goal-cell"> <strong>Support task completion flow recognition</strong> </td> <td class="actions-cell"> <ul class="action-list"> <li>Develop computer vision models for cooking state recognition</li> <li>Implement temporal reasoning for multi-step procedure tracking</li> <li>Create adaptive timing models based on user skill and environmental factors</li> </ul> </td> <td class="vision-cell"> Procedural assistance systems for manufacturing, healthcare protocols, educational labs, and repair/maintenance tasks. </td> </tr> <tr> <td class="goal-cell"> <strong>Accommodate diverse user queries and characteristics</strong> </td> <td class="actions-cell"> <ul class="action-list"> <li>Build multimodal interaction frameworks (voice, gesture, visual)</li> <li>Develop user modeling for skill adaptation and preference learning</li> <li>Create context-aware natural language understanding for domain-specific queries</li> </ul> </td> <td class="vision-cell"> Adaptive AI tutoring systems that accommodate different learning styles, physical abilities, and expertise levels across domains. </td> </tr> <tr> <td class="goal-cell"> <strong>Leverage multimodal sensing of environment</strong> </td> <td class="actions-cell"> <ul class="action-list"> <li>Integrate IoT sensors with computer vision and audio processing</li> <li>Develop sensor fusion algorithms for environmental state estimation</li> <li>Create privacy-preserving edge computing for real-time analysis</li> </ul> </td> <td class="vision-cell"> Smart environments that understand human activity and provide contextual assistance in homes, workplaces, and public spaces. </td> </tr> </tbody> </table> </div> </section> <section class="insight-section"> <div class="insight-content"> <div class="insight-icon">üí°</div> <div class="insight-text"> <h3>Reflection</h3> <p>This work demonstrates that effective AI assistance requires understanding not just user intent, but environmental context and task progression. The multimodal sensing approach and interaction patterns identified extend beyond cooking to any domain involving procedural guidance in physical environments.</p> <div class="insight-goals"> <div class="insight-goal"> <strong>Technical contribution:</strong> Multimodal fusion framework for real-time environmental understanding </div> <div class="insight-goal"> <strong>Interaction design:</strong> Context-aware query patterns that inform natural language interfaces </div> <div class="insight-goal"> <strong>Broader impact:</strong> Design principles for AI assistants in physical, procedural domains </div> </div> </div> </div> </section> </div> <style>:root{--primary-color:var(--global-theme-color,#2563eb);--primary-light:var(--global-theme-color-light,#93c5fd);--primary-dark:var(--global-theme-color-dark,#1d4ed8);--secondary-color:var(--global-secondary-color,#b509ac);--text-color:var(--global-text-color,#374151);--text-light:var(--global-text-color-light,#6b7280);--background-light:var(--global-bg-color-light,#f9fafb);--border-color:var(--global-divider-color,#e5e7eb);--success-color:var(--global-success-color,#059669);--card-bg:var(--global-card-bg-color,#fff);--gradient-bg:linear-gradient(135deg,var(--primary-color) 0%,var(--primary-dark) 100%);--hero-text-color:#fff;--card-text-color:#374151;--card-title-color:var(--primary-color);--insight-text-color:#fff;--warning-bg:#fef3c7;--warning-border:#f59e0b;--warning-text:#92400e}.portfolio-case-study{max-width:1200px;margin:0 auto;padding:0;font-family:var(--global-font-family,-apple-system,BlinkMacSystemFont,'Segoe UI',system-ui,sans-serif);line-height:1.6;color:var(--text-color)}.demo-section{margin-top:40px;padding-top:40px;border-top:2px solid rgba(255,255,255,0.2)}.demo-section h3{font-size:1.5rem;margin-bottom:25px;font-weight:500;opacity:.95;color:var(--hero-text-color)}.video-container{position:relative;width:100%;max-width:600px;margin:0 auto 25px;border-radius:12px;overflow:hidden;box-shadow:0 8px 32px rgba(0,0,0,0.3);background:rgba(255,255,255,0.1);backdrop-filter:blur(10px)}.video-container iframe{display:block;border-radius:12px}.tldr-hero{text-align:center;padding:60px 40px 80px;background:var(--gradient-bg);color:var(--hero-text-color);border-radius:12px;margin-bottom:60px;position:relative}.tldr-badge{display:inline-block;background:rgba(255,255,255,0.2);padding:8px 16px;border-radius:20px;font-weight:600;font-size:.9rem;margin-bottom:20px;backdrop-filter:blur(10px);color:var(--hero-text-color)}.hero-title{font-size:clamp(1.2rem,2.3vw,1.8rem);font-weight:700;margin-bottom:30px;line-height:1.2;color:var(--hero-text-color)}.hero-subtitle{max-width:800px;margin:0 auto;font-size:1.1rem;line-height:1.7;opacity:.95;color:var(--hero-text-color)}.hero-subtitle p,strong{margin-bottom:20px;color:var(--hero-text-color)}.metrics-strip{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:30px;padding:40px;background:var(--card-bg);border-radius:12px;box-shadow:0 2px 12px rgba(0,0,0,0.08);margin:-30px 20px 60px;position:relative;z-index:2;border:1px solid var(--border-color)}.metric-item{text-align:center}.metric-number{font-size:2.5rem;font-weight:700;color:var(--primary-color);line-height:1}.metric-label{font-size:.9rem;color:var(--text-light);margin-top:8px}.portfolio-case-study section{margin-bottom:80px}.section-header{display:flex;align-items:center;gap:12px;margin-bottom:40px}.section-header h2{font-size:2.2rem;font-weight:700;margin:0;color:var(--text-color)}.problem-content{display:grid;gap:40px}.problem-statement{background:var(--background-light);padding:40px;border-radius:16px;border-left:5px solid var(--primary-color)}.problem-statement h3{color:var(--primary-color);font-size:1.4rem;margin-bottom:20px;font-weight:600}.problem-details{display:grid;grid-template-columns:repeat(auto-fit,minmax(300px,1fr));gap:30px}.problem-point{background:white;padding:30px;border-radius:12px;box-shadow:0 2px 10px rgba(0,0,0,0.08);border:1px solid var(--border-color)}.problem-point h4{color:var(--primary-color);margin-bottom:12px;font-weight:600}.problem-section p{color:#374151!important}.role-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:24px}.role-card{background:var(--card-bg);padding:30px;border-radius:12px;text-align:center;box-shadow:0 2px 8px rgba(0,0,0,0.05);border:1px solid var(--border-color);transition:transform .2s ease,box-shadow .2s ease}.role-card:hover{transform:translateY(-2px);box-shadow:0 4px 16px rgba(0,0,0,0.1)}.role-icon{font-size:2.5rem;margin-bottom:16px}.role-card h4{color:var(--primary-color);margin-bottom:12px;font-weight:600}.approach-content{background:var(--card-bg);padding:40px;border-radius:12px;box-shadow:0 2px 8px rgba(0,0,0,0.05);border:1px solid var(--border-color)}.research-goal{background:var(--background-light);padding:28px;border-radius:12px;border-left:4px solid var(--primary-color);margin-bottom:40px}.research-goal h3{color:var(--primary-color);margin-bottom:16px;font-size:1.3rem;font-weight:600}.goal-text{font-size:1.1rem;margin-bottom:16px;font-weight:500;color:var(--card-text-color)}.goal-questions{list-style:none;padding:0;margin:0}.goal-questions li{padding:8px 0;position:relative;padding-left:20px;color:var(--card-text-color)}.goal-questions li:before{content:"‚Üí";color:var(--primary-color);font-weight:bold;position:absolute;left:0}.two-column{display:grid;grid-template-columns:1fr 1fr;gap:32px;margin:24px 0}.method-card{background:white;padding:24px;border-radius:12px;border:1px solid var(--border-color);box-shadow:0 2px 8px rgba(0,0,0,0.04)}.method-card p,.method-card p strong{color:var(--card-text-color)}
.method-title{font-weight:600;color:var(--primary-color);margin-bottom:12px;font-size:1.1rem}.stats-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(180px,1fr));gap:20px;margin:24px 0}.stat-card{background:white;padding:20px;border-radius:12px;text-align:center;border:1px solid var(--border-color);box-shadow:0 2px 6px rgba(0,0,0,0.04)}.stat-number{font-size:2.2rem;font-weight:700;color:var(--primary-color);display:block;line-height:1}.stat-label{font-size:.9rem;color:var(--card-text-color);margin-top:8px}.data-table{width:100%;border-collapse:collapse;margin:20px 0;background:white;border-radius:12px;overflow:hidden;box-shadow:0 2px 8px rgba(0,0,0,0.04)}.data-table th{background:var(--primary-color);color:white;padding:16px;text-align:left;font-weight:600;font-size:.95rem}.data-table td{padding:16px;border-bottom:1px solid var(--border-color);color:var(--card-text-color)}.data-table td strong{color:var(--card-text-color)}.data-table tr:last-child td{border-bottom:0}.data-table tr:nth-child(even){background:rgba(37,99,235,0.02)}.media-placeholder{background:var(--background-light);border:2px dashed var(--border-color);text-align:center;margin:24px 0;color:var(--text-light);display:flex;flex-direction:column;justify-content:center;align-items:center;gap:12px;border-radius:12px}.media-placeholder-large{min-height:300px}.media-placeholder img{width:100%;height:100%;object-fit:contain;border-radius:12px;display:block}.media-placeholder p{font-style:italic;font-size:.9rem;margin:8px 0 0 0}.design-decision{background:var(--warning-bg);padding:24px;border-radius:12px;border-left:4px solid var(--warning-border);margin:24px 0}.design-decision-label{font-weight:700;color:var(--warning-text);margin-bottom:12px;display:flex;align-items:center;gap:8px}.design-decision p,.design-decision strong{color:var(--warning-text);margin:0}.insights-grid{display:grid;gap:24px}.insight-card{background:white;padding:28px;border-radius:12px;border:1px solid var(--border-color);box-shadow:0 2px 8px rgba(0,0,0,0.04)}.insight-number{background:var(--primary-color);color:white;width:32px;height:32px;border-radius:50%;display:inline-flex;align-items:center;justify-content:center;font-weight:700;font-size:.9rem;margin-bottom:16px}.insight-title{font-size:1.1rem;font-weight:600;color:var(--primary-color);margin-bottom:16px}.insight-content p{color:var(--card-text-color);margin-bottom:16px}.insight-list{list-style:none;padding:0;margin:0}.insight-list li{padding:10px 0;border-bottom:1px solid var(--border-color);position:relative;padding-left:24px;color:var(--card-text-color)}.insight-list strong{color:var(--card-text-color)}.insight-list li:before{content:"‚úì";color:var(--success-color);font-weight:bold;position:absolute;left:0}.insight-list li:last-child{border-bottom:0}.analysis-section{margin-bottom:80px}.analysis-content{background:var(--card-bg);padding:40px;border-radius:12px;box-shadow:0 2px 8px rgba(0,0,0,0.05);border:1px solid var(--border-color)}.analysis-process{margin-top:40px}.process-step{display:flex;align-items:flex-start;gap:30px;margin-bottom:48px;padding-bottom:40px;border-bottom:1px solid var(--border-color)}.process-step:last-child{border-bottom:0;margin-bottom:0;padding-bottom:0}.step-number{background:var(--primary-color);color:white;width:60px;height:60px;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:1.2rem;flex-shrink:0}.step-content{flex:1}.step-content h4{margin-bottom:16px;color:var(--primary-color);font-size:1.2rem;font-weight:600}.step-content p{color:var(--text-light);margin-bottom:16px;line-height:1.6}.implications-table-container{background:var(--card-bg);border-radius:16px;box-shadow:0 4px 20px rgba(0,0,0,0.08);overflow:hidden;border:1px solid var(--border-color)}.implications-table{width:100%;border-collapse:collapse}.implications-table th{padding:24px 20px;text-align:left;font-weight:600;font-size:1.1rem}.implications-table td{padding:24px 20px;border-bottom:1px solid var(--border-color);vertical-align:top}.implications-table tr:last-child td{border-bottom:0}.implications-table tr:nth-child(even){background:rgba(37,99,235,0.02)}.goal-cell{border-right:1px solid var(--border-color);width:25%}.goal-cell strong{color:var(--primary-color);font-size:1.05rem;line-height:1.4}.actions-cell{width:40%;border-right:1px solid var(--border-color)}.vision-cell{width:35%;font-style:italic;color:var(--text-light);line-height:1.5}.action-list{list-style:none;padding:0;margin:0}.action-list li{padding:6px 0;position:relative;padding-left:20px;color:var(--text-light)}.action-list li:before{content:"‚Üí";color:var(--primary-color);font-weight:bold;position:absolute;left:0}.insight-section{background:var(--gradient-bg);color:white;border-radius:12px;padding:50px 40px;text-align:center}.insight-section *{color:white!important}.insight-content{display:flex;align-items:flex-start;gap:30px;max-width:800px;margin:0 auto;color:white}
.insight-icon{font-size:3rem;flex-shrink:0}.insight-text h3{margin-bottom:20px;font-size:1.5rem;font-weight:600}.insight-goals{display:grid;gap:15px;margin-top:25px;text-align:left}.insight-goal{background:rgba(255,255,255,0.1);padding:15px 20px;border-radius:12px;backdrop-filter:blur(10px)}@media(max-width:768px){.portfolio-case-study{padding:0 15px}.tldr-hero{padding:40px 20px 60px;margin-bottom:40px}.video-container{max-width:100%;margin:0 auto 20px}.metrics-strip{grid-template-columns:repeat(2,1fr);gap:20px;padding:30px 20px;margin:-20px 10px 40px}.insight-content{flex-direction:column;text-align:center}.two-column{grid-template-columns:1fr;gap:24px}.stats-grid{grid-template-columns:repeat(2,1fr)}.implications-table th,.implications-table td{padding:16px 12px;font-size:.9rem}.implications-table{font-size:.85rem}.goal-cell,.actions-cell,.vision-cell{width:auto;display:block}.implications-table tr{display:block;margin-bottom:20px;border:1px solid var(--border-color);border-radius:12px;overflow:hidden}.implications-table td{display:block;border-right:0;border-bottom:1px solid var(--border-color)}.implications-table td:last-child{border-bottom:0}.goal-cell{background:var(--primary-color)!important}.goal-cell strong{color:white!important}}@media(max-width:480px){.metrics-strip{grid-template-columns:1fr}.section-header{flex-direction:column;align-items:flex-start;gap:8px}}</style> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/companion-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/companion-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/companion-1400.webp"></source> <img src="/assets/img/publication_preview/companion.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="companion.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lin2023identifying" class="col-sm-8"> <div class="title">Identifying Multimodal Context Awareness Requirements for Supporting User Interaction with Procedural Videos</div> <div class="author"> <em>Georgianna Lin</em>,¬†Jin Yi Li,¬†Afsaneh Fazly, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Vladimir Pavlovic, Khai Truong' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3544548.3581006" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://drive.google.com/file/d/1uaQVkoxYTSXIsaWnY7Kg5Gd_u1JksHGw/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Following along how-to videos requires alternating focus between understanding procedural video instructions and performing them. Examining how to support these continuous context switches for the user has been largely unexplored. In this paper, we describe a user study with thirty participants who performed an hour-long cooking task while interacting with a wizard-of-oz hands-free interactive system that is aware of both their cooking progress and environment contexts. Through analysis of the session scripts, we identify a dichotomy between participant query differences and workflow alignment similarities, under-studied interactions that require AI functionality beyond video navigation alone, and queries that call for multimodal sensing of a user‚Äôs environment. By understanding the assistant experience through the participants‚Äô interactions, we identify design implications for a smart assistant that can discern a user‚Äôs task completion flow and personal characteristics, accommodate requests within and external to the task domain, andsupport nonvoice-based queries.</p> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Blue (Georgianna) Lin. Powered by </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>