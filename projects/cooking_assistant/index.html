<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Context Aware Cooking Assistant | Blue (Georgianna) Lin</title> <meta name="author" content="Blue (Georgianna) Lin"> <meta name="description" content="Designing a smart assistant to support continuous context switches in cooking by understanding user progress and environment through multimodal sensing"> <meta name="keywords" content="health sensing, ubiquitous devices, user-centered design, social good"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%B1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://glin39.github.io/projects/cooking_assistant/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Blue </span>(Georgianna) Lin</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Context Aware Cooking Assistant</h1> <p class="post-description">Designing a smart assistant to support continuous context switches in cooking by understanding user progress and environment through multimodal sensing</p> </header> <article> <div class="case-study"> <section class="hero-section"> <div class="hero-content"> <h2>The Problem</h2> <p>How might we design a context-aware assistant to support users as they switch focus between following procedural video instructions and performing cooking tasks?</p> </div> <div class="hero-image"> <img src="/assets/img/cooking.png" alt="Context Aware Cooking Assistant Interface" class="hero-image"> </div> </section> <section class="context-section"> <div class="row"> <div class="col-md-8"> <h3>Context &amp; Background</h3> <p>Following how-to cooking videos requires users to alternate their attention between understanding video instructions and performing those steps in real time. Supporting this continuous context switching with intelligent assistance has been largely unexplored. Current video navigation tools do not account for users' environment or progress, limiting their effectiveness.</p> </div> <div class="col-md-4"> <div class="project-details"> <h4>Project Details</h4> <ul class="project-info"> <li> <strong>Timeline:</strong> 2021 </li> <li> <strong>Role:</strong> Lead Researcher </li> <li> <strong>Methods:</strong> User study, wizard-of-oz prototyping, interaction analysis</li> </ul> </div> </div> </div> </section> <section class="process-section"> <h3>Research Process</h3> <div class="process-timeline"> <div class="process-step"> <div class="step-number">01</div> <div class="step-content"> <h4>User Study</h4> <p>Conducted a study with 30 participants performing an hour-long cooking task while interacting with a hands-free wizard-of-oz system aware of their cooking progress and environmental context.</p> </div> </div> <div class="process-step"> <div class="step-number">02</div> <div class="step-content"> <h4>Interaction Analysis</h4> <p>Analyzed session scripts to identify user query differences and workflow alignment similarities, revealing under-studied interaction needs beyond basic video navigation.</p> </div> </div> <div class="process-step"> <div class="step-number">03</div> <div class="step-content"> <h4>Design Implications</h4> <p>Derived design implications for an assistant capable of understanding task completion flow, personal user characteristics, and supporting nonvoice-based queries within and beyond the cooking domain.</p> </div> </div> </div> </section> <section class="study-section"> <h3>Study: Multimodal Context Awareness in Cooking</h3> <div class="row"> <div class="col-md-6"> <h4>Research Goal</h4> <p>Explore how a context-aware assistant can support users navigating video instructions and performing cooking tasks through multimodal sensing.</p> <h4>Key Findings</h4> <p>Identified a dichotomy between participant query differences and workflow alignment similarities, highlighted the need for AI beyond simple video navigation, and showed the importance of multimodal environmental sensing to support user queries.</p> </div> <div class="col-md-6"> <img src="/assets/img/cooking_gif.gif" alt="Cooking Assistant Study Visualization" class="img-fluid rounded"> </div> <iframe width="100%" height="400" src="https://www.youtube.com/embed/H57S8TY0hTA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""> </iframe> </div> </section> <section class="next-steps-section"> <h3>Design Implications &amp; Next Steps</h3> <table> <tr> <th>Design Goal</th> <th>Short-Term Actions</th> <th>Long-Term Vision</th> </tr> <tbody> <tr> <td>Support task completion flow recognition</td> <td> - Develop models to track cooking progress in real time<br> - Integrate progress awareness into assistant responses </td> <td> A cooking assistant that seamlessly adapts to user progress, providing timely and relevant guidance without disrupting task flow. </td> </tr> <tr> <td>Accommodate diverse user queries and characteristics</td> <td> - Enable handling of nonvoice queries<br> - Personalize assistant behavior based on user preferences and needs </td> <td> Adaptive AI that understands individual user traits, accommodates various interaction modes, and offers personalized support during cooking. </td> </tr> <tr> <td>Leverage multimodal sensing of environment</td> <td> - Implement environmental sensors (e.g., kitchen tools, gestures)<br> - Contextualize user queries with sensed environmental data </td> <td> A smart assistant that integrates multimodal context to provide accurate and proactive assistance beyond video navigation. </td> </tr> </tbody> </table> </section> <section class="impact-section"> <h3>Summary</h3> <div class="row"> <p>This research reveals the challenges users face when switching attention between video instructions and performing cooking tasks, emphasizing the need for context-aware assistants that understand both task progress and environmental cues.</p> <p>The findings motivate future design of smart cooking assistants that support natural interaction flows, accommodate diverse queries, and leverage multimodal sensing to enhance user experience and efficiency.</p> </div> &lt;/div&gt; </section> </div> <style>.case-study{max-width:1200px;margin:0 auto;padding:0 20px}.case-study section{margin-bottom:60px;padding-bottom:40px;border-bottom:1px solid var(--global-divider-color)}.case-study section:last-child{border-bottom:0}.hero-section{display:flex;align-items:flex-start;gap:40px;margin-bottom:80px}.hero-content h2{font-size:2.5rem;color:var(--global-text-color);margin-bottom:20px}.hero-content p{font-size:1.2rem;line-height:1.6;color:var(--global-text-color);opacity:.8}.hero-image img{max-width:600px;width:100%;height:auto;display:block;margin:0 auto}.project-details{background:var(--global-card-bg-color);padding:30px;border-radius:8px;border-left:4px solid var(--global-theme-color);border:1px solid var(--global-divider-color)}.project-info{list-style:none;padding:0;margin:0}.project-info li{margin-bottom:10px;padding-bottom:10px;border-bottom:1px solid var(--global-divider-color)}.project-info li:last-child{border-bottom:0}.process-timeline{display:flex;flex-direction:column;gap:40px}.process-step{display:flex;align-items:flex-start;gap:30px}.step-number{background:var(--global-theme-color);color:var(--global-bg-color);width:60px;height:60px;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;font-size:1.2rem;flex-shrink:0}.step-content h4{margin-bottom:10px;color:var(--global-text-color)}.study-section{background:var(--global-card-bg-color);padding:40px;border-radius:8px;margin-bottom:40px;border:1px solid var(--global-divider-color)}.study-section h4{color:var(--global-text-color);margin-bottom:20px}.solution-features{margin-top:40px}.feature{text-align:center;padding:20px}.feature h4{margin:20px 0 10px 0;color:var(--global-text-color)}.metrics{background:var(--global-card-bg-color);padding:30px;border-radius:8px;border-left:4px solid var(--global-theme-color);border:1px solid var(--global-divider-color)}.metrics ul{list-style:none;padding:0;margin:0}.metrics li{margin-bottom:10px;padding-bottom:10px;border-bottom:1px solid var(--global-divider-color)}.metrics li:last-child{border-bottom:0}.impact-section{background:linear-gradient(135deg,var(--global-theme-color) 0%,rgba(var(--global-theme-color-rgb),0.8) 100%);color:var(--global-bg-color);padding:60px 40px;border-radius:8px;margin:60px 0;border:1px solid var(--global-divider-color)}.next-steps-section{text-align:center;padding:40px;background:var(--global-card-bg-color);border-radius:8px;border:1px solid var(--global-divider-color)}@media(max-width:768px){.hero-section{flex-direction:column;text-align:center}.hero-content h2{font-size:2rem}.process-step{flex-direction:column;text-align:center}.step-number{margin:0 auto}}</style> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/companion-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/companion-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/companion-1400.webp"></source> <img src="/assets/img/publication_preview/companion.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="companion.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lin2023identifying" class="col-sm-8"> <div class="title">Identifying Multimodal Context Awareness Requirements for Supporting User Interaction with Procedural Videos</div> <div class="author"> Georgianna Lin, Jin Yi Li, Afsaneh Fazly, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Vladimir Pavlovic, Khai Truong' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3544548.3581006" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://drive.google.com/file/d/1uaQVkoxYTSXIsaWnY7Kg5Gd_u1JksHGw/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Following along how-to videos requires alternating focus between understanding procedural video instructions and performing them. Examining how to support these continuous context switches for the user has been largely unexplored. In this paper, we describe a user study with thirty participants who performed an hour-long cooking task while interacting with a wizard-of-oz hands-free interactive system that is aware of both their cooking progress and environment contexts. Through analysis of the session scripts, we identify a dichotomy between participant query differences and workflow alignment similarities, under-studied interactions that require AI functionality beyond video navigation alone, and queries that call for multimodal sensing of a user’s environment. By understanding the assistant experience through the participants’ interactions, we identify design implications for a smart assistant that can discern a user’s task completion flow and personal characteristics, accommodate requests within and external to the task domain, andsupport nonvoice-based queries.</p> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Blue (Georgianna) Lin. Powered by </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>